{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac691bd7-fcad-4b9e-b68b-b583536ae3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allow Jupyter to find the spark installation\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "#create a new spark session\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02480108-d02f-4b16-b1b9-4a60709afed3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.61:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1816e443680>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #run this cell to access the sparkui if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83dff526-4fb2-485b-bef4-a84d6d9e5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set your parameters\n",
    "\n",
    "server = 'database-1.choyskek6t2g.eu-north-1.rds.amazonaws.com' #data from files and the aggregations will be stored in this sql server\n",
    "port = '1433' #sql server port\n",
    "database = 'arqivaTechTest' #store data in this database\n",
    "workingTablePrefix = 'wrk_' #database working tables will have this prefix\n",
    "dbUser = 'rdsAccess' #the user you will use to read and write to the sql server\n",
    "dbPassword = 'xxxxxx' #the password for the dbUser - future work: pull this direct from AWS Secrets\n",
    "localStorage = 'C:\\\\temp\\\\arqiva\\\\' #files from s3 will be pulled to local storage to this location\n",
    "storageBucket = 'jameslester78-files' #files will land here and be archived here, in seperate folders\n",
    "\n",
    "\n",
    "#future work: remove need to escape slash in filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de90a419-1b46-4d74-b1a3-99d58bfef528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions required for project\n",
    "\n",
    "#future work: add in error handling and logging\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import current_date\n",
    "from pyspark.sql.functions import floor\n",
    "\n",
    "import boto3\n",
    "import pyodbc\n",
    "import re\n",
    "\n",
    "def getSqlTable(server,database,table,user,password):\n",
    "\n",
    "    #fetch a table from a sql server rds and load it into a dataframe\n",
    "    \n",
    "    df = spark.read.format(\"jdbc\").options(\n",
    "        url=f\"jdbc:sqlserver://{server}:{port};database={database};\", \n",
    "        driver=\"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        dbtable=f'{table}',\n",
    "        user=f\"{user}\",\n",
    "        password=f\"{password}\" #future work:this could be pulled from AWS secrets manager\n",
    "    ).load()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def writeWorkingTableToDb(server,port,database,workingTablePrefix,table,dbUser,dbPassword,df):\n",
    "\n",
    "    #create a working table on the rds\n",
    "    \n",
    "    df.write.format(\"jdbc\") \\\n",
    "          .mode('overwrite') \\\n",
    "          .option(\"url\", f\"jdbc:sqlserver://{server}:{1433};database={database};\") \\\n",
    "          .option(\"dbtable\", f'{workingTablePrefix}{table}') \\\n",
    "          .option(\"user\", f'{dbUser}') \\\n",
    "          .option(\"password\", f'{dbPassword}') \\\n",
    "          .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "          .save()\n",
    "    \n",
    "    return \n",
    "\n",
    "def createAggregationTable():\n",
    "\n",
    "    #Sum the transaction amount field for each user and write it to the RDS\n",
    "    \n",
    "    #pulling all data into a spark df, aggregating it then pushing it back to rds might not be optimal\n",
    "    #it may be better to do the aggregation on the rds, however we want to show some spark coding for this \n",
    "    #technical test\n",
    "\n",
    "    #fetch tables from rds into dataframes\n",
    "    transactions = getSqlTable(server,database,'transactions',dbUser,dbPassword)\n",
    "    users = getSqlTable(server,database,'users',dbUser,dbPassword)\n",
    "\n",
    "    #aggregate the transaction data frame\n",
    "    transactionsAgg = transactions.filter(\"EndDate is NULL\").groupBy(\"userFK\").agg(sum(\"amount\").alias(\"transactionSum\"))\n",
    "\n",
    "    #join the aggreated dataframe to the users dataframe and select the columns we are interested in\n",
    "    transSum = users.join(transactionsAgg, users.id == transactionsAgg.userFK, 'inner').select(users.id,users.name,transactionsAgg.transactionSum)\n",
    "\n",
    "    #for debugging only - show the resultant dataframe in the ui\n",
    "    transSum.show()\n",
    "\n",
    "    writeWorkingTableToDb (server,port,database,'','transSum',dbUser,dbPassword,transSum)\n",
    "\n",
    "def findFilesToProcess(bucketName,localStorage):\n",
    "\n",
    "    #download files from s3 to local storage\n",
    "    #return the local storeage filepath of downloaded files\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucketName)\n",
    "\n",
    "    #prepare lists to hold files\n",
    "    #future work: enable script to process multiple files at once if found in bucket\n",
    "    \n",
    "    userFiles = []\n",
    "    transactionsFiles = []\n",
    "\n",
    "    for obj in bucket.objects.all():\n",
    "        key = obj.key\n",
    "        body = obj.get()['Body'].read()\n",
    "\n",
    "        if key == 'unprocessed/users.csv':\n",
    "            userFiles.append(key)\n",
    "\n",
    "            file = open(localStorage + 'users.csv', 'w')\n",
    "            file.write(body.decode(\"utf-8\"))\n",
    "            file.close()\n",
    "            \n",
    "        if key == 'unprocessed/transactions.csv':\n",
    "            transactionsFiles.append(key) \n",
    "            \n",
    "            file = open(localStorage + 'transactions.csv', 'w')\n",
    "            file.write(body.decode(\"utf-8\"))\n",
    "            file.close()            \n",
    "            \n",
    "    return userFiles,transactionsFiles\n",
    "\n",
    "def createDataframeFromLocalFiles(fileList):\n",
    "\n",
    "    #convert file downloaded from s3 into dataframes\n",
    "    \n",
    "    returnList = []\n",
    "    \n",
    "    for list in fileList:\n",
    "        for ele in list:\n",
    "            localLoc = localStorage + ele.replace('unprocessed/','')\n",
    "            df = spark.read.format(\"csv\").option(\"header\", \"true\").load(localLoc)\n",
    "            returnList.append((ele,localLoc,df))\n",
    "    \n",
    "    return returnList\n",
    "\n",
    "def transformFiles(createDataframeFromLocalFilesReturned):\n",
    "\n",
    "    transformedData = []  \n",
    "    \n",
    "    #calculate age, remove any user under 18 years old\n",
    "    #cast datatypes where not string\n",
    "    #return processed dataframes\n",
    "    \n",
    "    for file in createDataframeFromLocalFilesReturned:\n",
    "\n",
    "        if file[0] == 'unprocessed/users.csv':\n",
    "            \n",
    "            df = file[2]\n",
    "            df = df.withColumn(\"date_of_birth\",to_timestamp(col(\"date_of_birth\"), 'dd/MM/yyyy')) #convert dob to timestamp\n",
    "            df = df.withColumn(\"age\",floor(datediff(current_date(),col(\"date_of_birth\"))/365))  #calculate age in years\n",
    "            df = df.filter(col(\"age\") > 18)  #remove users under 18\n",
    "            df = df.drop(\"age\")  #no need to retain the age working column \n",
    "            df = df.withColumn(\"user_id\",col(\"user_id\").cast(\"integer\"))  #convert field to int\n",
    "            transformedData.append ((file[0],file[1],file[2],df))   \n",
    "    \n",
    "        if file[0] == 'unprocessed/transactions.csv':\n",
    "            \n",
    "            df = file[2]\n",
    "            df = df.withColumn(\"transaction_date\",to_timestamp(col(\"transaction_date\"), 'dd/MM/yyyy')) #convert txn date to timestamp\n",
    "            df = df.withColumn(\"amount\",col(\"amount\").cast(\"integer\")) #convert field to int     \n",
    "            df = df.withColumn(\"transaction_id\",col(\"transaction_id\").cast(\"integer\")) #convert field to int     \n",
    "            df = df.withColumn(\"user_id\",col(\"user_id\").cast(\"integer\")) #convert field to int     \n",
    "            transformedData.append ((file[0],file[1],file[2],df))\n",
    "\n",
    "    return (transformedData)\n",
    "\n",
    "\n",
    "def updateSDC2Tables():\n",
    "\n",
    "    #update the scd2 tables\n",
    "    \n",
    "    #in order to avoid FK errors:\n",
    "    #inserts first, user table followed by transactions\n",
    "    #delete transactions next, then users\n",
    "    #finally updates\n",
    "    \n",
    "    #data should be loaded as scd2, which means we need a new column (startdate and enddate)\n",
    "    #add new field id for surrogte key and fk field in txns table\n",
    "    \n",
    "    deleteUsers =   \"update users \"\\\n",
    "                    \"set endDate = getdate() \"\\\n",
    "                    \"from users a \"\\\n",
    "                    \"inner join wrk_users b on a.user_id = b.user_id \"\\\n",
    "                    \"where b.type = 'd' and a.endDate IS NULL\"\n",
    "    \n",
    "    insertUsers =   \"insert into users (user_id,name,email,date_of_birth) \"\\\n",
    "                    \"select user_id,name,email,date_of_birth \"\\\n",
    "                    \"from wrk_users \"\\\n",
    "                    \"where type = 'i'\"\n",
    "    \n",
    "    updateUser  =   \"update users \"\\\n",
    "                    \"set endDate = getdate() \"\\\n",
    "                    \"from users a \"\\\n",
    "                    \"inner join wrk_users b on a.user_id = b.user_id  \"\\\n",
    "                    \"where b.type = 'u'  and a.endDate IS NULL \"\\\n",
    "                    \"insert into users ( user_id,name,email,date_of_birth) select user_id,name,email,date_of_birth  \"\\\n",
    "                    \"from wrk_users \"\\\n",
    "                    \"where type = 'u'  \"\n",
    "    \n",
    "    #end any transactions where the user has been deleted\n",
    "    \n",
    "    endtxns =           \"update transactions \"\\\n",
    "                        \"set EndDate = getdate() \"\\\n",
    "                        \"from transactions a \"\\\n",
    "                        \"inner join wrk_users b on a.user_id = b.user_id \"\\\n",
    "                        \"where type = 'd'\"\n",
    "    \n",
    "    #update FKs for amended users who how have new surrogate PKs\n",
    "    \n",
    "    updateTxnUserFK =   \"update transactions \"\\\n",
    "                        \"set EndDate = getdate() \"\\\n",
    "                        \"from transactions a \"\\\n",
    "                        \"inner join wrk_users b on a.user_id = b.user_id \"\\\n",
    "                        \"inner join (select transaction_id,max(id) id from transactions group by transaction_id) c on c.id = a.id \"\\\n",
    "                        \"where type = 'u' \"\\\n",
    "                        \"\\n\"\\\n",
    "                        \"insert into transactions (userFk,transaction_id,user_id,amount,transaction_date) \"\\\n",
    "                        \"select c.id,a.transaction_id,a.user_id,a.amount,a.transaction_date \"\\\n",
    "                        \"from transactions a \"\\\n",
    "                        \"inner join wrk_users b on a.user_id = b.user_id \"\\\n",
    "                        \"inner join (select user_id,max(id) id from users group by user_id) c on b.user_id = c.user_id \"\\\n",
    "                        \"inner join (select transaction_id,max(id) id from transactions group by transaction_id) d on d.id = a.id \"\\\n",
    "                        \"where type = 'u' \"\n",
    "    \n",
    "    #update transaction table with transaction table changes\n",
    "    \n",
    "    deleteTransactions =    \"update transactions \"\\\n",
    "                            \"set endDate = getdate() \"\\\n",
    "                            \"from wrk_transactions a  \"\\\n",
    "                            \"inner join (select transaction_id,max(transaction_id) id from transactions group by transaction_id) b on a.transaction_id = b.transaction_id \"\\\n",
    "                            \"where a.type = 'd'  \"\n",
    "    \n",
    "    insertTransaction =     \"insert into transactions(transaction_id,user_id,amount,transaction_date,userFK)  \"\\\n",
    "                            \"select transaction_id,a.user_id,amount,transaction_date,b.id \"\\\n",
    "                            \"from wrk_transactions  a \"\\\n",
    "                            \"inner join (select user_id,max(id) id from users group by user_id) b on a.user_id = b.user_id \"\\\n",
    "                            \"where type = 'i' \"\n",
    "    \n",
    "    \n",
    "    updateTransaction =     \"update transactions  \"\\\n",
    "                            \"set endDate = getdate()  \"\\\n",
    "                            \"from (select transaction_id,max(id) id from transactions group by transaction_id) a  \"\\\n",
    "                            \"inner join wrk_transactions b on a.transaction_id = b.transaction_id   \"\\\n",
    "                            \"inner join transactions d on d.transaction_id = a.transaction_id \"\\\n",
    "                            \"where b.type = 'u'   \"\\\n",
    "                            \"\\n  \"\\\n",
    "                            \"insert into transactions (transaction_id,user_id,amount,transaction_date,userFK) \"\\\n",
    "                            \"select transaction_id,a.user_id,amount,transaction_date,b.id \"\\\n",
    "                            \"from wrk_transactions a \"\\\n",
    "                            \"inner join (select user_id,max(id) id from users group by user_id) b on a.user_id = b.user_id \"\\\n",
    "                            \"where type = 'u'  \"\n",
    "    \n",
    "    connectString = 'Driver={ODBC Driver 17 for SQL Server};Server='\n",
    "    connectString += f'{server};Database={database};Uid={dbUser};Pwd={dbPassword};'\n",
    "    \n",
    "    cnxn  = pyodbc.connect(connectString, autocommit=False) \n",
    "    crsr = cnxn.cursor()\n",
    "    crsr.execute(insertUsers)\n",
    "    crsr.execute(deleteUsers)\n",
    "    crsr.execute(updateUser)\n",
    "    crsr.execute(endtxns)\n",
    "    crsr.execute(updateTxnUserFK)\n",
    "    crsr.execute(deleteTransactions)\n",
    "    crsr.execute(insertTransaction)\n",
    "    crsr.execute(updateTransaction)\n",
    "    cnxn.commit()\n",
    "    cnxn.close()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def moveS3File (source,destination,sourceBucket):\n",
    "\n",
    "    #move files from unprocessed folder to processed folder\n",
    "    \n",
    "    #get datetime for archive suffix\n",
    "    now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    copy_source = {\n",
    "        'Bucket': f'{sourceBucket}',\n",
    "        'Key': f'{source}'\n",
    "    }    \n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(sourceBucket)\n",
    "\n",
    "    #using regular expresiion add a timestamp to the filename before the period\n",
    "    \n",
    "    destination = re.sub(r'(\\.)', rf'_{now}\\1', destination)\n",
    "\n",
    "    bucket.copy(copy_source, destination)\n",
    "\n",
    "    s3.Object(sourceBucket,source).delete()\n",
    "    \n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73b6300-2cd9-49c9-b683-c14a3872ef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------------+-------------------+----+\n",
      "|user_id| name|          email|      date_of_birth|type|\n",
      "+-------+-----+---------------+-------------------+----+\n",
      "|      1|James|james@gmail.com|1978-09-13 00:00:00|   i|\n",
      "|      2|Craig|  craig@aol.com|1972-06-02 00:00:00|   i|\n",
      "+-------+-----+---------------+-------------------+----+\n",
      "\n",
      "+--------------+-------+------+-------------------+----+\n",
      "|transaction_id|user_id|amount|   transaction_date|type|\n",
      "+--------------+-------+------+-------------------+----+\n",
      "|             4|      1|   170|2024-07-02 00:00:00|   u|\n",
      "|             5|      3|    10|2024-07-03 00:00:00|   i|\n",
      "|             6|      4|     6|2024-07-03 00:00:00|   i|\n",
      "+--------------+-------+------+-------------------+----+\n",
      "\n",
      "+---+-----+--------------+\n",
      "| id| name|transactionSum|\n",
      "+---+-----+--------------+\n",
      "|126|James|         80.00|\n",
      "|125|  Jay|         24.00|\n",
      "|131|James|        170.00|\n",
      "|124|Lydia|         40.00|\n",
      "+---+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#execute steps for a pair of user and transaction files\n",
    "\n",
    "outputFiles = findFilesToProcess(storageBucket,localStorage) #download files from S3\n",
    "createDataframeFromLocalFilesReturned = createDataframeFromLocalFiles(outputFiles) #create dataframes\n",
    "transformedFilesReturned = transformFiles(createDataframeFromLocalFilesReturned) #transform dataframes\n",
    "\n",
    "#view dataframes\n",
    "users  = transformedFilesReturned[0][-1] \n",
    "transactions = transformedFilesReturned[1][-1]\n",
    "users.show()\n",
    "transactions.show()\n",
    "\n",
    "#write dataframes to rds\n",
    "writeWorkingTableToDb (server,port,database,workingTablePrefix,'users',dbUser,dbPassword,users)\n",
    "writeWorkingTableToDb (server,port,database,workingTablePrefix,'transactions',dbUser,dbPassword,transactions)\n",
    "\n",
    "#update the slowly changing dimension tables\n",
    "updateSDC2Tables()\n",
    "\n",
    "#refresh the aggregation table\n",
    "createAggregationTable()\n",
    "\n",
    "#archive the files (move them to the processed folder and timestamp them)\n",
    "moveS3File('unprocessed/users.csv','processed/users.csv','jameslester78-files')\n",
    "moveS3File('unprocessed/transactions.csv','processed/transactions.csv','jameslester78-files')\n",
    "\n",
    "\n",
    "#future work - data quality checks - eg each user id has only one userFK in transaction table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
